# Half Cheetah with Proximal Policy Optimization (PPO)

Python version used: `3.12`.